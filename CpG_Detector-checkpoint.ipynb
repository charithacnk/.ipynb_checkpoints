{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "\n",
    "def prepare_data(num_samples=100):\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    #step2\n",
    "    temp = [intseq_to_dnaseq(seq) for seq in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in X_dna_seqs_train] # use count_cpgs here to generate labels with temp generated in step2\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some config\n",
    "LSTM_HIDDEN = 16\n",
    "LSTM_LAYER = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_x_tensor = torch.tensor(train_x)\n",
    "test_x_tensor = torch.tensor(test_x)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)  # Convert labels to floats\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32)  # Convert labels to floats\n",
    "\n",
    "# create data loader\n",
    "train_data_loader =  DataLoader(\n",
    "    dataset=torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_Data_loader = DataLoader(\n",
    "    dataset=torch.utils.data.TensorDataset(test_x_tensor, test_y_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No shuffling needed for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        # TODO complete model, you are free to add whatever layers you need here\n",
    "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO complete forward function\n",
    "        embeddings = nn.functional.one_hot(x, num_classes=5).float()\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = lstm_out[:, -1, :]\n",
    "        \n",
    "        #Predict CpG count using the linear layer\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "import torch.nn as nn\n",
    "model = CpGPredictor(5,32)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09571299851813819\n",
      "0.0010617807572543825\n",
      "0.00022028727994438668\n",
      "0.00012347974484328006\n",
      "9.209139722088366e-05\n",
      "7.721985838315959e-05\n",
      "6.787969937249727e-05\n",
      "5.9024541371854866e-05\n",
      "5.296557836231841e-05\n",
      "4.561747141451633e-05\n"
     ]
    }
   ],
   "source": [
    "# training (you can modify the code below)\n",
    "t_loss = 0.0\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "for epoch in range(epoch_num):\n",
    "    for data, target in train_data_loader:\n",
    "        #TODO complete training loop\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output.squeeze(), target)\n",
    "        \n",
    "        t_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch loss\n",
    "        t_loss += loss.item()\n",
    "        \n",
    "    print(t_loss)\n",
    "    t_loss = .0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "res_gs = []  # Store ground truth CpG counts\n",
    "res_pred = []  # Store predicted CpG counts\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    for data, target in test_Data_loader:\n",
    "        output = model(data)  # Pass data through the model\n",
    "\n",
    "        # Detach and convert outputs for comparison and storage\n",
    "        predicted = output.squeeze().detach().cpu().numpy().tolist()\n",
    "        target = target.detach().cpu().numpy().tolist()\n",
    "\n",
    "        res_gs.extend(target)  # Append ground truth counts\n",
    "        res_pred.extend(predicted)  # Append predicted counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.414942978099439e-07\n",
      "Mean Absolute Error: 0.00048632130346959457\n",
      "R-Squared: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreen\\AppData\\Local\\Temp\\ipykernel_22752\\3988613988.py:21: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - (ss_res / ss_tot)\n"
     ]
    }
   ],
   "source": [
    "# TODO complete evaluation of the model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    y_true_np = np.array(y_true)\n",
    "    y_pred_np = np.array(y_pred)\n",
    "    return np.mean(np.square(y_true_np - y_pred_np))\n",
    "\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    y_true_np = np.array(y_true)\n",
    "    y_pred_np = np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true_np - y_pred_np))\n",
    "\n",
    "def calculate_r_squared(y_true, y_pred):\n",
    "    y_true_np = np.array(y_true)\n",
    "    y_pred_np = np.array(y_pred)\n",
    "    ss_res = np.sum(np.square(y_true_np - y_pred_np))\n",
    "    ss_tot = np.sum(np.square(y_true_np - np.mean(y_true_np)))\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "\n",
    "# Example using  data collected from 'res_gs' and 'res_pred' during inference\n",
    "mse = calculate_mse(res_gs, res_pred)\n",
    "mae = calculate_mae(res_gs, res_pred)\n",
    "r_squared = calculate_r_squared(res_gs, res_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-Squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMrRf_aVDRJm"
   },
   "source": [
    "# Part 2: what if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AKvG-MNuXJr9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "random.seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    # Step 1: Generate random DNA sequences\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    \n",
    "    # Step 2: Pad sequences to a common length\n",
    "    max_seq_len = max(len(seq) for seq in X_dna_seqs_train)\n",
    "    padded_seqs = []\n",
    "    for seq in X_dna_seqs_train:\n",
    "        padded_seq = seq + [0] * (max_seq_len - len(seq))  # Pad with zeros\n",
    "        padded_seqs.append(padded_seq)\n",
    "\n",
    "    # Convert to tensors\n",
    "    padded_seqs_tensor = torch.tensor(padded_seqs, dtype=torch.long)\n",
    "    \n",
    "    # Step 3: Count CpG dinucleotides for target labels\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in X_dna_seqs_train]\n",
    "\n",
    "    # Step 4: Convert integer sequences to tensors (optional)\n",
    "    if torch.cuda.is_available():\n",
    "        padded_seqs_tensor = padded_seqs_tensor.cuda()\n",
    "\n",
    "    return padded_seqs_tensor, y_dna_seqs\n",
    "\n",
    "# Example usage\n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lists, labels) -> None:\n",
    "        self.lists = lists\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists)\n",
    "\n",
    "    \n",
    "# this will be a collate_fn for dataloader to pad sequence  \n",
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "\n",
    "    # Extract sequences and labels from the batch\n",
    "        sequences, labels = zip(*batch)\n",
    "\n",
    "    # Pad sequences to a common length (assume padding value 0)\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Convert labels to tensors (if necessary)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return padded_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(train_x, train_y)\n",
    "test_data = MyDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=PadSequence())\n",
    "test_loader = DataLoader(test_data, batch_size=32, collate_fn=PadSequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 30 (1845672750.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 32\u001b[1;36m\u001b[0m\n\u001b[1;33m    return torch.tensor(converted_sequence, dtype=torch.long)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 30\n"
     ]
    }
   ],
   "source": [
    "# TODO complete the rest\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Import your data preparation functions (replace with actual implementation)\n",
    "#from DataLoader import prepare_data, MyDataset, PadSequence\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5  # Number of unique characters in DNA alphabet (N, A, C, G, T)\n",
    "hidden_size = 64  # Number of hidden units in LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "learning_rate = 0.001  # Learning rate for optimizer\n",
    "num_epochs = 10  # Number of training epochs\n",
    "\n",
    "# Model definition\n",
    "class CpGPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # Embedding layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output layer for predicting CpG count\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert DNA sequence to integer encoding (replace with your encoding logic)\n",
    "        encoded_x = torch.tensor(dnaseq_to_intseq(x), dtype=torch.long)\n",
    "        def dnaseq_to_intseq(seq):\n",
    "  # ... conversion logic ...\n",
    "        return torch.tensor(converted_sequence, dtype=torch.long)\n",
    "        print(f\"Dimension of encoded_x: {len(encoded_x)}\")\n",
    "    if len(encoded_x) == 1:  # Check if only one dimension\n",
    "        x_tensor = torch.tensor(encoded_x, dtype=torch.long).to(device)\n",
    "    else:\n",
    "  # Add batch dimension if necessary\n",
    "        x_tensor = torch.tensor(encoded_x).unsqueeze(0).to(device)\n",
    "    \n",
    "        # Create a tensor from the encoded sequence\n",
    "        x_tensor = torch.tensor(encoded_x).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        # Pass through embedding layer\n",
    "        embeddings = self.embedding(x_tensor)\n",
    "\n",
    "        # Pass through LSTM layers\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        # Extract the last hidden state (assuming batch-first ordering)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "        # Predict CpG count with linear layer\n",
    "        prediction = self.fc(last_hidden)\n",
    "        return prediction\n",
    "\n",
    "# Prepare data\n",
    "train_x, train_y = prepare_data(2048, 64, 128)\n",
    "test_x, test_y = prepare_data(512, 64, 128)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_data = MyDataset(train_x, train_y)\n",
    "test_data = MyDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=PadSequence())\n",
    "test_loader = DataLoader(test_data, batch_size=32, collate_fn=PadSequence())\n",
    "\n",
    "# Determine device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CpGPredictor(input_size, hidden_size, num_layers).to(device)\n",
    "loss_fn = nn.MSELoss()  # Mean squared error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.float())  # Convert target to float for MSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print('Average Training Loss: {:.6f}'.format(avg_loss))\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
